{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = 'C:\\\\Users\\\\xxxx\\\\Desktop\\\\bcg_carCrash_caseStudy\\\\venvbcg\\\\Scripts\\\\python.exe'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'C:\\\\Users\\\\xxxx\\\\Desktop\\\\bcg_carCrash_caseStudy\\\\venvbcg\\\\Scripts\\\\python.exe'\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\hadoop\\\\hadoop-3.3.6'\n",
    "os.environ['SPARK_HOME'] = 'C:\\\\Spark\\\\spark-3.5.3-bin-hadoop3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "charges_schema = StructType([\n",
    "    StructField(\"CRASH_ID\", IntegerType(), True),\n",
    "    StructField(\"UNIT_NBR\", IntegerType(), True),\n",
    "    StructField(\"PRSN_NBR\", IntegerType(), True),\n",
    "    StructField(\"CHARGE\", StringType(), True),\n",
    "    StructField(\"CITATION_NBR\", StringType(), True)\n",
    "])\n",
    "\n",
    "damages_schema = StructType([\n",
    "    StructField(\"CRASH_ID\", IntegerType(), True),\n",
    "    StructField(\"DAMAGED_PROPERTY\", StringType(), True)\n",
    "])\n",
    "\n",
    "endorse_schema = StructType([\n",
    "    StructField(\"CRASH_ID\", IntegerType(), True),\n",
    "    StructField(\"UNIT_NBR\", IntegerType(), True),\n",
    "    StructField(\"DRVR_LIC_ENDORS_ID\", StringType(), True)\n",
    "])\n",
    "\n",
    "primary_person_schema = StructType([\n",
    "    StructField(\"CRASH_ID\", IntegerType(), True),\n",
    "    StructField(\"UNIT_NBR\", IntegerType(), True),\n",
    "    StructField(\"PRSN_NBR\", IntegerType(), True),\n",
    "    StructField(\"PRSN_TYPE_ID\", StringType(), True),\n",
    "    StructField(\"PRSN_OCCPNT_POS_ID\", StringType(), True),\n",
    "    StructField(\"PRSN_INJRY_SEV_ID\", StringType(), True),\n",
    "    StructField(\"PRSN_AGE\", FloatType(), True),\n",
    "    StructField(\"PRSN_ETHNICITY_ID\", StringType(), True),\n",
    "    StructField(\"PRSN_GNDR_ID\", StringType(), True),\n",
    "    StructField(\"PRSN_EJCT_ID\", StringType(), True),\n",
    "    StructField(\"PRSN_REST_ID\", StringType(), True),\n",
    "    StructField(\"PRSN_AIRBAG_ID\", StringType(), True),\n",
    "    StructField(\"PRSN_HELMET_ID\", StringType(), True),\n",
    "    StructField(\"PRSN_SOL_FL\", StringType(), True),\n",
    "    StructField(\"PRSN_ALC_SPEC_TYPE_ID\", StringType(), True),\n",
    "    StructField(\"PRSN_ALC_RSLT_ID\", StringType(), True),\n",
    "    StructField(\"PRSN_BAC_TEST_RSLT\", FloatType(), True),\n",
    "    StructField(\"PRSN_DRG_SPEC_TYPE_ID\", StringType(), True),\n",
    "    StructField(\"PRSN_DRG_RSLT_ID\", StringType(), True),\n",
    "    StructField(\"DRVR_DRG_CAT_1_ID\", StringType(), True),\n",
    "    StructField(\"PRSN_DEATH_TIME\", StringType(), True),\n",
    "    StructField(\"INCAP_INJRY_CNT\", IntegerType(), True),\n",
    "    StructField(\"NONINCAP_INJRY_CNT\", IntegerType(), True),\n",
    "    StructField(\"POSS_INJRY_CNT\", IntegerType(), True),\n",
    "    StructField(\"NON_INJRY_CNT\", IntegerType(), True),\n",
    "    StructField(\"UNKN_INJRY_CNT\", IntegerType(), True),\n",
    "    StructField(\"TOT_INJRY_CNT\", IntegerType(), True),\n",
    "    StructField(\"DEATH_CNT\", IntegerType(), True),\n",
    "    StructField(\"DRVR_LIC_TYPE_ID\", StringType(), True),\n",
    "    StructField(\"DRVR_LIC_STATE_ID\", StringType(), True),\n",
    "    StructField(\"DRVR_LIC_CLS_ID\", StringType(), True),\n",
    "    StructField(\"DRVR_ZIP\", StringType(), True)\n",
    "])\n",
    "\n",
    "restrict_schema = StructType([\n",
    "    StructField(\"CRASH_ID\", IntegerType(), True),\n",
    "    StructField(\"UNIT_NBR\", IntegerType(), True),\n",
    "    StructField(\"DRVR_LIC_RESTRIC_ID\", StringType(), True)\n",
    "])\n",
    "\n",
    "units_schema = StructType([\n",
    "    StructField(\"CRASH_ID\", IntegerType(), True),\n",
    "    StructField(\"UNIT_NBR\", IntegerType(), True),\n",
    "    StructField(\"UNIT_DESC_ID\", StringType(), True),\n",
    "    StructField(\"VEH_PARKED_FL\", StringType(), True),\n",
    "    StructField(\"VEH_HNR_FL\", StringType(), True),\n",
    "    StructField(\"VEH_LIC_STATE_ID\", StringType(), True),\n",
    "    StructField(\"VIN\", StringType(), True),\n",
    "    StructField(\"VEH_MOD_YEAR\", FloatType(), True),\n",
    "    StructField(\"VEH_COLOR_ID\", StringType(), True),\n",
    "    StructField(\"VEH_MAKE_ID\", StringType(), True),\n",
    "    StructField(\"VEH_MOD_ID\", StringType(), True),\n",
    "    StructField(\"VEH_BODY_STYL_ID\", StringType(), True),\n",
    "    StructField(\"EMER_RESPNDR_FL\", StringType(), True),\n",
    "    StructField(\"OWNR_ZIP\", StringType(), True),\n",
    "    StructField(\"FIN_RESP_PROOF_ID\", StringType(), True),\n",
    "    StructField(\"FIN_RESP_TYPE_ID\", StringType(), True),\n",
    "    StructField(\"VEH_DMAG_AREA_1_ID\", StringType(), True),\n",
    "    StructField(\"VEH_DMAG_SCL_1_ID\", StringType(), True),\n",
    "    StructField(\"FORCE_DIR_1_ID\", FloatType(), True),\n",
    "    StructField(\"VEH_DMAG_AREA_2_ID\", StringType(), True),\n",
    "    StructField(\"VEH_DMAG_SCL_2_ID\", StringType(), True),\n",
    "    StructField(\"FORCE_DIR_2_ID\", FloatType(), True),\n",
    "    StructField(\"VEH_INVENTORIED_FL\", StringType(), True),\n",
    "    StructField(\"VEH_TRANSP_NAME\", StringType(), True),\n",
    "    StructField(\"VEH_TRANSP_DEST\", StringType(), True),\n",
    "    StructField(\"CONTRIB_FACTR_1_ID\", StringType(), True),\n",
    "    StructField(\"CONTRIB_FACTR_2_ID\", StringType(), True),\n",
    "    StructField(\"CONTRIB_FACTR_P1_ID\", StringType(), True),\n",
    "    StructField(\"VEH_TRVL_DIR_ID\", StringType(), True),\n",
    "    StructField(\"FIRST_HARM_EVT_INV_ID\", StringType(), True),\n",
    "    StructField(\"INCAP_INJRY_CNT\", IntegerType(), True),\n",
    "    StructField(\"NONINCAP_INJRY_CNT\", IntegerType(), True),\n",
    "    StructField(\"POSS_INJRY_CNT\", IntegerType(), True),\n",
    "    StructField(\"NON_INJRY_CNT\", IntegerType(), True),\n",
    "    StructField(\"UNKN_INJRY_CNT\", IntegerType(), True),\n",
    "    StructField(\"TOT_INJRY_CNT\", IntegerType(), True),\n",
    "    StructField(\"DEATH_CNT\", IntegerType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(\n",
    "            os.path.dirname(os.getcwd()), \"data\", \"input\")\n",
    "output_path = os.path.join(\n",
    "            os.path.dirname(os.getcwd()), \"data\", \"output\")\n",
    "\n",
    "primary_person_path = os.path.join(input_path, \"Primary_Person_use.csv\")\n",
    "units_path = os.path.join(input_path, \"Units_use.csv\")\n",
    "damages_path = os.path.join(input_path, \"Damages_use.csv\")\n",
    "charges_path = os.path.join(input_path, \"Charges_use.csv\")\n",
    "endorse_path = os.path.join(input_path, \"Endorse_use.csv\")\n",
    "restricts_path = os.path.join(input_path, \"Restrict_use.csv\")\n",
    "\n",
    "output_one_path = os.path.join(output_path, \"Output_one.csv\")\n",
    "output_two_path = os.path.join(output_path, \"Output_two.csv\")\n",
    "output_three_path = os.path.join(output_path, \"Output_three.csv\")\n",
    "output_four_path = os.path.join(output_path, \"Output_four.csv\")\n",
    "output_five_path = os.path.join(output_path, \"Output_five.csv\")\n",
    "output_six_path = os.path.join(output_path, \"Output_six.csv\")\n",
    "output_seven_path = os.path.join(output_path, \"Output_seven.csv\")\n",
    "output_eight_path = os.path.join(output_path, \"Output_eight.csv\")\n",
    "output_nine_path = os.path.join(output_path, \"Output_nine.csv\")\n",
    "output_ten_path = os.path.join(output_path, \"Output_ten.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Analytics 1: Find the number of crashes (accidents) in which number of males killed are greater than 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of crashes in which more than 2 males were killed are: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the primary person data with the schema\n",
    "primary_person_df = spark.read.csv(primary_person_path, header=True, schema=primary_person_schema)\n",
    "\n",
    "# Step 1: Filter records where the gender is male and death count is greater than zero\n",
    "males_killed_df = primary_person_df.filter((col(\"PRSN_GNDR_ID\") == \"MALE\") & (primary_person_df['PRSN_INJRY_SEV_ID'] == \"KILLED\"))\n",
    "\n",
    "# Step 2: Group by CRASH_ID and sum the DEATH_CNT\n",
    "males_killed_by_crash_df = males_killed_df.groupBy(\"CRASH_ID\").agg(sum(\"DEATH_CNT\").alias(\"total_male_deaths\"))\n",
    "\n",
    "# Step 3: Filter the crashes where the number of male deaths is greater than 2\n",
    "crashes_with_more_than_two_males_killed_df = males_killed_by_crash_df.filter(col(\"total_male_deaths\") > 2)\n",
    "\n",
    "# Step 4: Count the number of distinct CRASH_IDs in above df\n",
    "number_of_crashes = crashes_with_more_than_two_males_killed_df.select(\"CRASH_ID\").count()\n",
    "\n",
    "# Display the result\n",
    "print(f\"The number of crashes in which more than 2 males were killed are: {number_of_crashes}\")\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "output_df = spark.createDataFrame([(number_of_crashes,)], [\"Number_of_Crashes_2+_male_deaths\"])\n",
    "output_df.write.csv(output_one_path, mode=\"overwrite\", header=True)\n",
    "\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|Number_of_Crashes_2+_male_deaths|\n",
      "+--------------------------------+\n",
      "|                               0|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "output_one_df = spark.read.csv(output_one_path, header=True)\n",
    "output_one_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Analysis 2: How many two wheelers are booked for crashes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Two Wheelers that are booked for crashes: 784\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the units data with the schema\n",
    "units_df = spark.read.csv(units_path, header=True, schema=units_schema)\n",
    "# units_df.show()\n",
    "\n",
    "# Step 1: Filter for two-wheelers\n",
    "two_wheelers_df = units_df.filter(\n",
    "    col(\"VEH_BODY_STYL_ID\").isin([\"MOTORCYCLE\", \"POLICE MOTORCYCLE\"])).select(\"CRASH_ID\")\n",
    "# two_wheelers_df.show()\n",
    "\n",
    "# Step 2: Count the number of unique crashes\n",
    "number_of_two_wheeler_crashes = two_wheelers_df.count()\n",
    "\n",
    "# Display the result\n",
    "print(f\"The number of Two Wheelers that are booked for crashes: {number_of_two_wheeler_crashes}\")\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "output_df = spark.createDataFrame([(number_of_two_wheeler_crashes,)], [\"Number_of_Two_Wheelers_Crashed\"])\n",
    "output_df.write.csv(output_two_path, mode=\"overwrite\", header=True)\n",
    "\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|Number_of_Two_Wheelers_Crashed|\n",
      "+------------------------------+\n",
      "|                           784|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "output_two_df = spark.read.csv(output_two_path, header=True)\n",
    "output_two_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Analysis 3: Determine the Top 5 Vehicle Makes of the cars present in the crashes in which driver died and Airbags did not deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the primary person and units data with the schema\n",
    "primary_person_df = spark.read.csv(primary_person_path, header=True, schema=primary_person_schema)\n",
    "units_df = spark.read.csv(units_path, header=True, schema=units_schema)\n",
    "\n",
    "# Step 1: Filter the Primary_Person data for driver deaths with no airbags deployed\n",
    "driver_deaths_df = primary_person_df.filter((col(\"PRSN_TYPE_ID\") == \"DRIVER\") & \n",
    "                                            (primary_person_df['PRSN_INJRY_SEV_ID'] == \"KILLED\") & \n",
    "                                            (primary_person_df['PRSN_AIRBAG_ID'] == \"NOT DEPLOYED\")).select(\n",
    "                                                \"CRASH_ID\", \"UNIT_NBR\"\n",
    "                                            )\n",
    "# driver_deaths_df.show()\n",
    "\n",
    "# Step 2: Join the filtered Primary_Person data with the Units data\n",
    "joined_df = driver_deaths_df.join(\n",
    "    units_df, \n",
    "    (driver_deaths_df.CRASH_ID == units_df.CRASH_ID) & \n",
    "    (driver_deaths_df.UNIT_NBR == units_df.UNIT_NBR), \n",
    "    \"inner\"\n",
    ").select( \"VEH_BODY_STYL_ID\", \"VEH_MAKE_ID\")\n",
    "# joined_df.show()\n",
    "\n",
    "# Step 3: Filter for cars only\n",
    "cars_without_airbag_deployment_df = joined_df.filter(\n",
    "    col(\"VEH_BODY_STYL_ID\").isin([\"PASSENGER CAR, 4-DOOR\", \"SPORT UTILITY VEHICLE\", \"PASSENGER CAR, 2-DOOR\", \"VAN\", \"POLICE CAR/TRUCK\"])\n",
    ")\n",
    "# cars_without_airbag_deployment_df.show()\n",
    "\n",
    "# Step 4: Group by VEH_MAKE_ID, count occurrences, and sort to get top 5\n",
    "top5_vehicle_makes_df = cars_without_airbag_deployment_df.groupBy(\"VEH_MAKE_ID\").count().orderBy(col(\"count\").desc()).limit(5)\n",
    "top5_vehicle_makes_df = top5_vehicle_makes_df.select(col(\"VEH_MAKE_ID\").alias(\"VEH_MAKER\"), col(\"count\").alias(\"Number_of_Vehicles\"))\n",
    "# top5_vehicle_makes_df.show()\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "top5_vehicle_makes_df.write.csv(output_three_path, mode=\"overwrite\", header=True)\n",
    "\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|VEH_MAKER|Number_of_Vehicles|\n",
      "+---------+------------------+\n",
      "|CHEVROLET|                 5|\n",
      "|   NISSAN|                 4|\n",
      "|     FORD|                 3|\n",
      "|    HONDA|                 2|\n",
      "| CADILLAC|                 1|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "output_three_df = spark.read.csv(output_three_path, header=True)\n",
    "output_three_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Analysis 4: Determine number of Vehicles with driver having valid licences involved in hit and run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Vehicles that are involved with Hit and Run but driven by Licensed Drivers: 2609\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the primary person and units data with the schema\n",
    "primary_person_df = spark.read.csv(primary_person_path, header=True, schema=primary_person_schema)\n",
    "units_df = spark.read.csv(units_path, header=True, schema=units_schema)\n",
    "\n",
    "# Step 1: Filter the Primary_Person data for valid licenses\n",
    "valid_license_df = primary_person_df.filter(\n",
    "    col(\"DRVR_LIC_TYPE_ID\").isin([\"DRIVER LICENSE\", \"COMMERCIAL DRIVER LIC.\"])\n",
    ").select(\"CRASH_ID\", \"UNIT_NBR\", \"DRVR_LIC_TYPE_ID\")\n",
    "# valid_license_df.show()\n",
    "\n",
    "# Step 2: Filter the Units data for hit-and-run incidents\n",
    "hit_and_run_df = units_df.filter(col(\"VEH_HNR_FL\") == \"Y\").select(\"CRASH_ID\", \"UNIT_NBR\", \"VEH_HNR_FL\")\n",
    "# hit_and_run_df.show()\n",
    "\n",
    "# Step 3: Join the filtered data on CRASH_ID and UNIT_NBR\n",
    "joined_df = valid_license_df.join(\n",
    "    hit_and_run_df, \n",
    "    (valid_license_df.CRASH_ID == hit_and_run_df.CRASH_ID) & \n",
    "    (valid_license_df.UNIT_NBR == hit_and_run_df.UNIT_NBR), \n",
    "    \"inner\"\n",
    ")\n",
    "# joined_df.show()\n",
    "\n",
    "# Step 4: Count the number of vehicles involved\n",
    "vehicle_count = joined_df.count()\n",
    "\n",
    "# Display the result\n",
    "print(f\"The number of Vehicles that are involved with Hit and Run but driven by Licensed Drivers: {vehicle_count}\")\n",
    "\n",
    "# Save the result to a CSV file\n",
    "result_df = spark.createDataFrame([(vehicle_count,)], [\"Number_of_Vehicles_HnR_Licensed\"])\n",
    "result_df.write.csv(output_four_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|Number_of_Vehicles_HnR_Licensed|\n",
      "+-------------------------------+\n",
      "|                           2609|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "output_four_df = spark.read.csv(output_four_path, header=True)\n",
    "output_four_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Analysis 5: Which state has highest number of accidents in which females are not involved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, first\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the primary person data with the schema\n",
    "primary_person_df = spark.read.csv(primary_person_path, header=True, schema=primary_person_schema)\n",
    "\n",
    "# Step 1: Filter out records where the person is a female\n",
    "females_df = primary_person_df.filter(col(\"PRSN_GNDR_ID\") == \"FEMALE\")\n",
    "# females_df.show()\n",
    "\n",
    "# Step 2: List out crashes where females are involved\n",
    "crashes_female_df = females_df.select(\"CRASH_ID\").distinct()\n",
    "# crashes_female_df.show()\n",
    "\n",
    "# Step 3: Filter out crashes involving females from the original data\n",
    "no_female_crashes_df = primary_person_df.join(crashes_female_df, on=\"CRASH_ID\", how=\"left_anti\")\n",
    "# no_female_crashes_df.show()\n",
    "\n",
    "# Step 4: Group by CRASH_ID and get state (driver's state)\n",
    "crashes_by_state = no_female_crashes_df.groupBy(\"CRASH_ID\").agg(first(\"DRVR_LIC_STATE_ID\").alias(\"DRIVER_STATE\"))\n",
    "# crashes_by_state.show()\n",
    "\n",
    "# Step 5: Count number of crashes grouped by state\n",
    "state_crash_count = crashes_by_state.groupBy(\"DRIVER_STATE\").count()\n",
    "# state_crash_count.show()\n",
    "\n",
    "# Step 6: Find the state with the highest number of crashes\n",
    "# state_with_max_crashes = state_crash_count.orderBy(col(\"count\").desc())\n",
    "state_with_max_crashes = state_crash_count.orderBy(col(\"count\").desc()).limit(1)\n",
    "state_with_max_crashes = state_with_max_crashes.select(col(\"DRIVER_STATE\").alias(\"STATE_with_max_no_F_crashes\"), col(\"count\").alias(\"Number_of_Crashes\"))\n",
    "# state_with_max_crashes.show()\n",
    "\n",
    "# Save the result to a CSV file\n",
    "state_with_max_crashes.write.csv(output_five_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-----------------+\n",
      "|STATE_with_max_no_F_crashes|Number_of_Crashes|\n",
      "+---------------------------+-----------------+\n",
      "|                      Texas|            32326|\n",
      "+---------------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "output_five_df = spark.read.csv(output_five_path, header=True)\n",
    "output_five_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Analysis 6: Which are the Top 3rd to 5th VEH_MAKE_IDs that contribute to a largest number of injuries including death"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "- (INCAP_INJRY_CNT) + (NONINCAP_INJRY_CNT) + (POSS_INJRY_CNT) + (UNKN_INJRY_CNT) == TOT_INJRY_CNT\n",
    "- NON_INJRY_CNT\n",
    "- DEATH_CNT\n",
    "\n",
    "- DEATH_CNT + TOT_INJRY_CNT = TOTAL_INJURIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the units data with the schema\n",
    "units_df = spark.read.csv(units_path, header=True, schema=units_schema)\n",
    "\n",
    "# Step 1: Calculate the total number of injuries including deaths\n",
    "injuries_df = units_df.withColumn(\n",
    "    \"TOTAL_INJURIES\",\n",
    "    col(\"TOT_INJRY_CNT\") + col(\"DEATH_CNT\")\n",
    ").select(\"CRASH_ID\", \"UNIT_NBR\", \"VEH_MAKE_ID\", \"TOT_INJRY_CNT\", \"DEATH_CNT\", \"TOTAL_INJURIES\")\n",
    "# injuries_df.show()\n",
    "\n",
    "# Step 2: Group by VEH_MAKE_ID and sum the TOTAL_INJURIES\n",
    "vehicle_injury_counts = injuries_df.groupBy(\"VEH_MAKE_ID\").agg(\n",
    "    sum(\"TOTAL_INJURIES\").alias(\"TOTAL_INJURIES_SUM\")\n",
    ")\n",
    "# vehicle_injury_counts.show()\n",
    "\n",
    "# Step 3: Sort the VEH_MAKE_IDs by total injuries in descending order\n",
    "sorted_vehicle_injuries = vehicle_injury_counts.orderBy(col(\"TOTAL_INJURIES_SUM\").desc())\n",
    "# sorted_vehicle_injuries.show()\n",
    "\n",
    "# Step 4: Select the 3rd to 5th VEH_MAKE_IDs\n",
    "top_3rd_to_5th_vehicle_makes = sorted_vehicle_injuries.limit(5).tail(3)\n",
    "top_3rd_to_5th_vehicle_makes\n",
    "\n",
    "# Output the result\n",
    "top_3rd_to_5th_vehicle_makes_df = spark.createDataFrame(top_3rd_to_5th_vehicle_makes)\n",
    "# top_3rd_to_5th_vehicle_makes_df.show()\n",
    "\n",
    "# Save the result to a CSV file\n",
    "top_3rd_to_5th_vehicle_makes_df.write.csv(output_six_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|VEH_MAKE_ID|TOTAL_INJURIES_SUM|\n",
      "+-----------+------------------+\n",
      "|     TOYOTA|              4228|\n",
      "|     NISSAN|              3118|\n",
      "|      DODGE|              3146|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "output_six_df = spark.read.csv(output_six_path, header=True)\n",
    "output_six_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Analysis 7: For all the body styles involved in crashes, mention the top ethnic user group of each unique body style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, count, row_number\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the primary person and units data with the schema\n",
    "primary_person_df = spark.read.csv(primary_person_path, header=True, schema=primary_person_schema)\n",
    "units_df = spark.read.csv(units_path, header=True, schema=units_schema)\n",
    "\n",
    "# Step 1: Join units_df and primary_person_df on CRASH_ID and UNIT_NBR\n",
    "joined_df = units_df.join(primary_person_df, on=[\"CRASH_ID\", \"UNIT_NBR\"], how=\"inner\")\n",
    "\n",
    "# Step 2: Group by VEH_BODY_STYL_ID and PRSN_ETHNICITY_ID, and count occurrences\n",
    "ethnic_group_counts = joined_df.groupBy(\"VEH_BODY_STYL_ID\", \"PRSN_ETHNICITY_ID\").agg(\n",
    "    count(\"*\").alias(\"ETHNIC_GROUP_COUNT\")\n",
    ")\n",
    "# ethnic_group_counts.show()\n",
    "\n",
    "# Step 3: Use Window function to rank ethnic groups for each body style\n",
    "window_spec = Window.partitionBy(\"VEH_BODY_STYL_ID\").orderBy(col(\"ETHNIC_GROUP_COUNT\").desc())\n",
    "ranked_ethnic_groups = ethnic_group_counts.withColumn(\n",
    "    \"rank\", \n",
    "    row_number().over(window_spec)\n",
    ")\n",
    "# ranked_ethnic_groups.show()\n",
    "\n",
    "# Step 4: Filter to get the top ethnic group for each body style (rank = 1)\n",
    "top_ethnic_user_group = ranked_ethnic_groups.filter(col(\"rank\") == 1).select(\n",
    "    \"VEH_BODY_STYL_ID\", \n",
    "    \"PRSN_ETHNICITY_ID\"\n",
    ")\n",
    "# top_ethnic_user_group.show()\n",
    "\n",
    "# Save the result to a CSV file\n",
    "top_ethnic_user_group.write.csv(output_seven_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|    VEH_BODY_STYL_ID|PRSN_ETHNICITY_ID|\n",
      "+--------------------+-----------------+\n",
      "|           AMBULANCE|            WHITE|\n",
      "|                 BUS|         HISPANIC|\n",
      "|      FARM EQUIPMENT|            WHITE|\n",
      "|          FIRE TRUCK|            WHITE|\n",
      "|          MOTORCYCLE|            WHITE|\n",
      "|                  NA|            WHITE|\n",
      "|NEV-NEIGHBORHOOD ...|            WHITE|\n",
      "|        NOT REPORTED|            WHITE|\n",
      "|OTHER  (EXPLAIN I...|            WHITE|\n",
      "|PASSENGER CAR, 2-...|            WHITE|\n",
      "|PASSENGER CAR, 4-...|            WHITE|\n",
      "|              PICKUP|            WHITE|\n",
      "|    POLICE CAR/TRUCK|            WHITE|\n",
      "|   POLICE MOTORCYCLE|            WHITE|\n",
      "|SPORT UTILITY VEH...|            WHITE|\n",
      "|               TRUCK|            WHITE|\n",
      "|       TRUCK TRACTOR|            WHITE|\n",
      "|             UNKNOWN|          UNKNOWN|\n",
      "|                 VAN|            WHITE|\n",
      "|   YELLOW SCHOOL BUS|            BLACK|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "output_seven_df = spark.read.csv(output_seven_path, header=True)\n",
    "output_seven_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Analysis 8: Among the crashed cars, what are the Top 5 Zip Codes with highest number crashes with alcohols as the contributing factor to a crash (Use Driver Zip Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the primary person data with the schema\n",
    "primary_person_df = spark.read.csv(primary_person_path, header=True, schema=primary_person_schema)\n",
    "\n",
    "# Step 1: Filter the data for alcohol as a contributing factor\n",
    "alcohol_related_crashes = primary_person_df.filter(\n",
    "    (col(\"PRSN_ALC_RSLT_ID\") == \"Positive\") & (col(\"PRSN_TYPE_ID\") == \"DRIVER\")).select(\n",
    "        \"CRASH_ID\", \"PRSN_ALC_RSLT_ID\", \"PRSN_TYPE_ID\", \"DRVR_ZIP\"\n",
    "    )\n",
    "# alcohol_related_crashes.show()\n",
    "\n",
    "# Step 2: Group by the driver's zip code and count the number of crashes\n",
    "crash_count_by_zip = alcohol_related_crashes.groupBy(\"DRVR_ZIP\").agg(count(\"CRASH_ID\").alias(\"CRASH_COUNT\"))\n",
    "# crash_count_by_zip.show()\n",
    "\n",
    "# Step 3: Sort by crash count in descending order and select the top 5 zip codes\n",
    "top_5_zip_codes = crash_count_by_zip.orderBy(col(\"CRASH_COUNT\").desc()).limit(6).tail(5)\n",
    "top_5_zip_codes\n",
    "\n",
    "# Output the result\n",
    "top_5_zip_codes_df = spark.createDataFrame(top_5_zip_codes)\n",
    "# top_5_zip_codes_df.show()\n",
    "\n",
    "# Save the result to a CSV file\n",
    "top_5_zip_codes_df.write.csv(output_eight_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|DRVR_ZIP|CRASH_COUNT|\n",
      "+--------+-----------+\n",
      "|   78521|         61|\n",
      "|   76010|         48|\n",
      "|   79936|         42|\n",
      "|   79938|         37|\n",
      "|   78550|         33|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "output_eight_df = spark.read.csv(output_eight_path, header=True)\n",
    "output_eight_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Analysis 9: Count of Distinct Crash IDs where No Damaged Property was observed and Damage Level (VEH_DMAG_SCL~) is above 4 and car avails Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of distinct Crash IDs meeting all criteria: 8784\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the units and damages data with the schema\n",
    "units_df = spark.read.csv(units_path, header=True, schema=units_schema)\n",
    "damages_df = spark.read.csv(damages_path, header=True, schema=damages_schema)\n",
    "\n",
    "# Step 1: Filter for records with damaged property observed\n",
    "# it is damaged_df database \n",
    "\n",
    "# Step 2: Filter for damage levels above 4 (DAMAGED 5 or higher)\n",
    "damage_level_df = units_df.filter(\n",
    "    (col(\"VEH_DMAG_SCL_1_ID\").isin([\"DAMAGED 5\", \"DAMAGED 6\", \"DAMAGED 7 HIGHEST\"])) |\n",
    "    (col(\"VEH_DMAG_SCL_2_ID\").isin([\"DAMAGED 5\", \"DAMAGED 6\", \"DAMAGED 7 HIGHEST\"]))\n",
    ")\n",
    "# damage_level_df.show()\n",
    "\n",
    "# Step 3: Join no_damaged_property_df with damage_level_df\n",
    "damage_and_no_property_df = damage_level_df.join(damages_df, \"CRASH_ID\", \"left_anti\")\n",
    "# damage_and_no_property_df.show()\n",
    "\n",
    "# Step 4: Filter for cars with insurance\n",
    "damaged_insured_df = damage_and_no_property_df.filter(col(\"FIN_RESP_TYPE_ID\").isin([\"PROOF OF LIABILITY INSURANCE\", \"LIABILITY INSURANCE POLICY\"]))\n",
    "# damaged_insured_df.show()\n",
    "\n",
    "# Step 5: Count distinct CRASH_IDs\n",
    "distinct_crash_count = damaged_insured_df.select(\"CRASH_ID\").distinct().count()\n",
    "\n",
    "print(\"Count of distinct Crash IDs meeting all criteria:\", distinct_crash_count)\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "output_df = spark.createDataFrame([(distinct_crash_count,)], [\"Number_of_Crashes\"])\n",
    "output_df.write.csv(output_nine_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Number_of_Crashes|\n",
      "+-----------------+\n",
      "|             8784|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "output_nine_df = spark.read.csv(output_nine_path, header=True)\n",
    "output_nine_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Analysis 10: Determine the Top 5 Vehicle Makes where drivers are charged with speeding related offences, has licensed Drivers, used top 10 used vehicle colours and has car licensed with the Top 25 states with highest number of offences (to be deduced from the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, desc, first, count\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CrashAnalysis1\").getOrCreate()\n",
    "\n",
    "# Load the differnt data tables with the schema\n",
    "charges_df = spark.read.csv(charges_path, header=True, schema=charges_schema)\n",
    "primary_person_df = spark.read.csv(primary_person_path, header=True, schema=primary_person_schema)\n",
    "units_df = spark.read.csv(units_path, header=True, schema=units_schema)\n",
    "\n",
    "\n",
    "# Step 1: Filter for speeding-related offences\n",
    "speeding_offences_df = charges_df.filter(lower(col(\"CHARGE\")).contains(\"speed\"))\n",
    "# speeding_offences_df.show()\n",
    "\n",
    "# Step 2: Filter for licensed drivers\n",
    "licensed_drivers_df = primary_person_df.filter(col(\"DRVR_LIC_TYPE_ID\").isin([\"DRIVER LICENSE\", \"COMMERCIAL DRIVER LIC.\"]))\n",
    "# licensed_drivers_df.show()\n",
    "\n",
    "# Step 3: Find the top 10 most common vehicle colors\n",
    "top_10_colors = units_df.groupBy(\"VEH_COLOR_ID\").count().orderBy(desc(\"count\")).limit(11)\n",
    "# top_10_colors.show()\n",
    "top_10_color_list = [row[\"VEH_COLOR_ID\"] for row in top_10_colors.collect() if row[\"VEH_COLOR_ID\"] != 'NA']\n",
    "# top_10_color_list\n",
    "\n",
    "\n",
    "# Step 4: Determine the top 25 states with the highest number of offences\n",
    "# Aggregate the charges by CRASH_ID\n",
    "charges_agg = charges_df.groupBy(\"CRASH_ID\").count()\n",
    "# charges_agg.show()\n",
    "\n",
    "# Group by CRASH_ID and get state (driver's state)\n",
    "crashes_by_state = licensed_drivers_df.groupBy(\"CRASH_ID\").agg(first(\"DRVR_LIC_STATE_ID\").alias(\"DRIVER_STATE\"))\n",
    "# crashes_by_state.show()\n",
    "\n",
    "#  Join with primary_person_df to get the driver's license state\n",
    "offenses_by_state = charges_agg.join(\n",
    "    primary_person_df, on=\"CRASH_ID\", how=\"inner\").groupBy(\n",
    "        \"DRVR_LIC_STATE_ID\").agg(count(\"CRASH_ID\").alias(\n",
    "            \"offense_count\")).orderBy(desc(\"offense_count\")).limit(28)\n",
    "# offenses_by_state.show()\n",
    "\n",
    "top_25_states_list = [row.DRVR_LIC_STATE_ID for row in offenses_by_state\n",
    "    .filter(~col(\"DRVR_LIC_STATE_ID\").isin(\"NA\", \"Other\",\"Unknown\"))\n",
    "    .collect()]\n",
    "# top_25_states_list\n",
    "\n",
    "# Step 5: Filter units_df for cars with one of the top 10 colors\n",
    "filtered_units_df = units_df.filter(\n",
    "    (col(\"VEH_COLOR_ID\").isin(top_10_color_list))\n",
    ")\n",
    "# filtered_units_df.show()\n",
    "\n",
    "# Step 6: Join data to get licensed drivers with speeding offences and License stste in top 25 states\n",
    "filtered_drivers_df = speeding_offences_df.join(licensed_drivers_df, on=\"CRASH_ID\", how=\"inner\").filter(col(\"DRVR_LIC_STATE_ID\").isin(top_25_states_list))\n",
    "# filtered_drivers_df.show()                                                           \n",
    "\n",
    "# Step 7: Join above two dataframes to get required dataframe\n",
    "final_df = filtered_drivers_df.join(filtered_units_df, on=[\"CRASH_ID\", \"UNIT_NBR\"], how=\"inner\")\n",
    "# final_df.show()\n",
    "\n",
    "# Step 5: Determine the top 5 vehicle makes\n",
    "top_vehicle_makes = final_df.groupBy(\"VEH_MAKE_ID\").agg(count(\"CRASH_ID\").alias(\"offense_count\")).orderBy(desc(\"offense_count\")).limit(5)\n",
    "# top_vehicle_makes.show()\n",
    "\n",
    "# Save the result to a CSV file\n",
    "top_vehicle_makes.write.csv(output_ten_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|VEH_MAKE_ID|offense_count|\n",
      "+-----------+-------------+\n",
      "|       FORD|         8564|\n",
      "|  CHEVROLET|         7455|\n",
      "|     TOYOTA|         4601|\n",
      "|      DODGE|         3763|\n",
      "|      HONDA|         2854|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CrashAnalysis\").getOrCreate()\n",
    "\n",
    "output_ten_df = spark.read.csv(output_ten_path, header=True)\n",
    "output_ten_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvbcg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
